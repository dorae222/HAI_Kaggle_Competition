{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd38d6f1-9b53-4b0c-bf3d-c486f1750871",
   "metadata": {},
   "source": [
    "## 학습된 모델 가중치 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8537391-73ad-4f2a-83fa-4e41e318dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62151b0f-ba62-4ded-b492-f23a66fc2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    if len(preds.shape) > 1 and preds.shape[1] > 1:\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    else:\n",
    "        pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec83115-a4b3-43fe-a9d6-adec794309ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(file_path, tokenizer, args):\n",
    "\n",
    "    def get_input_ids(data):\n",
    "        document_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in data]\n",
    "        tokenized_texts = [tokenizer.tokenize(s) for s in tqdm(document_bert, \"Tokenizing\")]\n",
    "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(tokenized_texts, \"Converting tokens to ids\")]\n",
    "        print(\"Padding sequences...\")\n",
    "        input_ids = pad_sequences(input_ids, maxlen=args.maxlen, dtype='long', truncating='post', padding='post')\n",
    "        return input_ids\n",
    "\n",
    "    def get_attention_masks(input_ids):\n",
    "        attention_masks = []\n",
    "        for seq in tqdm(input_ids, \"Generating attention masks\"):\n",
    "            seq_mask = [float(i > 0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return attention_masks\n",
    "\n",
    "    def get_data_loader(inputs, masks, labels, batch_size=args.batch):\n",
    "        data = TensorDataset(torch.tensor(inputs), torch.tensor(masks), torch.tensor(labels))\n",
    "        sampler = RandomSampler(data) if args.mode == 'train' else SequentialSampler(data)\n",
    "        data_loader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "        return data_loader\n",
    "\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    input_ids = get_input_ids(data_df['text'].values)\n",
    "    attention_masks = get_attention_masks(input_ids)\n",
    "    labels = data_df['label'].values  # 수정된 부분\n",
    "    data_loader = get_data_loader(input_ids, attention_masks, labels)\n",
    "\n",
    "    return data_loader, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f3e755-8413-4438-bdb1-3a54119d4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, args, data_loader):\n",
    "    print('start predict')\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = []\n",
    "    eval_accuracy = []\n",
    "    logits = []\n",
    "\n",
    "    for step, batch in tqdm(enumerate(data_loader)):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.mode == 'test':\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "                logit = outputs[0]\n",
    "            else:\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss, logit = outputs[:2]\n",
    "                eval_loss.append(loss.item())\n",
    "\n",
    "        logit = logit.detach().cpu().numpy()\n",
    "        label = b_labels.cpu().numpy()\n",
    "\n",
    "        logits.append(logit)\n",
    "\n",
    "        if args.mode != 'test':\n",
    "            accuracy = flat_accuracy(logit, label)\n",
    "            eval_accuracy.append(accuracy)\n",
    "\n",
    "    logits = np.vstack(logits)\n",
    "    predict_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "    if args.mode == 'test':\n",
    "        return predict_labels, None\n",
    "\n",
    "    avg_eval_loss = np.mean(eval_loss)\n",
    "    avg_eval_accuracy = np.mean(eval_accuracy)\n",
    "\n",
    "    return predict_labels, avg_eval_loss, avg_eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400ca488-fb7d-4bf0-bb58-1efa08297594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████████████████████| 973/973 [00:00<00:00, 24849.04it/s]\n",
      "Converting tokens to ids: 100%|███████████| 973/973 [00:00<00:00, 247351.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating attention masks: 100%|█████████| 973/973 [00:00<00:00, 152757.07it/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01,  9.97it/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.15it/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.23it/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.12it/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 1의 검증 정확도: 0.94140625\n",
      "모델 2의 검증 정확도: 0.9385516826923077\n",
      "모델 3의 검증 정확도: 0.9404296875\n",
      "모델 4의 검증 정확도: 0.9404296875\n",
      "모델 5의 검증 정확도: 0.935546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개 모델 선택\n",
    "top_n = 5\n",
    "\n",
    "# 검증 정확도를 기준으로 내림차순 정렬하고 상위 N개 선택\n",
    "df = pd.read_csv(\"./hparams_table.csv\")\n",
    "df\n",
    "\n",
    "top_models = df.nlargest(top_n, 'best_val_accuracy')\n",
    "\n",
    "# 선택된 모델들의 가중치 파일 경로 생성\n",
    "model_paths = [f\"best_models/model{i+1}.pth\" for i in top_models.index]\n",
    "\n",
    "model_paths\n",
    "\n",
    "\n",
    "# 파라미터 설정\n",
    "args = easydict.EasyDict({\n",
    "    \"valid_path\": \"./valid.csv\",\n",
    "    \"device\" : 'cpu',\n",
    "    \"mode\" : \"valid\",\n",
    "    \"batch\" : 64,\n",
    "    \"maxlen\" : 32,\n",
    "    \"model_ckpt\" : \"monologg/koelectra-small-v3-discriminator\",\n",
    "})\n",
    "\n",
    "# 토크나이저 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_ckpt)\n",
    "\n",
    "# 검증 데이터 로더 생성\n",
    "valid_dataloader, valid_labels = generate_data_loader(args.valid_path, tokenizer, args)\n",
    "\n",
    "# 각 모델에 대한 성능을 저장할 리스트\n",
    "all_accuracies = []\n",
    "\n",
    "# 모든 모델에 대해 반복\n",
    "for model_path in model_paths:\n",
    "    # 모델 생성\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_ckpt, num_labels=3)\n",
    "    # 모델 가중치 로드\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # 모델 예측 및 정확도 계산\n",
    "    _, _, avg_eval_accuracy = predict(model, args, valid_dataloader)\n",
    "    all_accuracies.append(avg_eval_accuracy)\n",
    "\n",
    "for i, acc in enumerate(all_accuracies):\n",
    "    print(f'모델 {i+1}의 검증 정확도: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9d51c-1095-4e41-835e-ae6bba1c977f",
   "metadata": {},
   "source": [
    "## 하드 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aea1286-2ace-4e96-abfc-25dd20f6c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, args, data_loader):\n",
    "    print('start predict')\n",
    "    all_logits = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model_logits = []\n",
    "\n",
    "        for step, batch in tqdm(enumerate(data_loader)):\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "                logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            model_logits.append(logits)\n",
    "\n",
    "        model_logits = np.vstack(model_logits)\n",
    "        all_logits.append(model_logits)\n",
    "\n",
    "    all_logits = np.stack(all_logits)\n",
    "    ensemble_logits = np.sum(all_logits, axis=0)\n",
    "    ensemble_preds = np.argmax(ensemble_logits, axis=1)\n",
    "\n",
    "    return ensemble_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c57f31-b4d0-4a2f-bf1f-410b0086e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.65it/s]\n",
      "16it [00:01, 10.59it/s]\n",
      "16it [00:01, 10.65it/s]\n",
      "16it [00:01, 10.67it/s]\n",
      "16it [00:01, 10.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블의 검증 정확도: 0.9383350462487153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "# 모든 모델에 대해 반복\n",
    "for model_path in model_paths:\n",
    "    # 모델 생성\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_ckpt, num_labels=3)\n",
    "    # 모델 가중치 로드\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(args.device)\n",
    "    models.append(model)\n",
    "\n",
    "# 앙상블 예측\n",
    "ensemble_predictions = ensemble_predict(models, args, valid_dataloader)\n",
    "\n",
    "# 앙상블 예측의 정확도 계산\n",
    "accuracy = flat_accuracy(ensemble_predictions, valid_labels)\n",
    "print(f'앙상블의 검증 정확도: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fbfd4-e4ed-4299-82a7-d4d3c5d02d06",
   "metadata": {},
   "source": [
    "## 소프트 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73b4153-9884-41ee-943e-df9a433499e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, args, data_loader):\n",
    "    print('start predict')\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    for step, batch in tqdm(enumerate(data_loader)):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        all_logits.append(logits)\n",
    "\n",
    "    all_logits = np.vstack(all_logits)\n",
    "    predict_proba = np.exp(all_logits) / np.sum(np.exp(all_logits), axis=1, keepdims=True)\n",
    "\n",
    "    return predict_proba\n",
    "\n",
    "def ensemble_predict(models, args, data_loader):\n",
    "    print('start predict')\n",
    "    all_predict_proba = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model_predict_proba = predict_proba(model, args, data_loader)\n",
    "        all_predict_proba.append(model_predict_proba)\n",
    "\n",
    "    all_predict_proba = np.array(all_predict_proba)\n",
    "    ensemble_predict_proba = np.mean(all_predict_proba, axis=0)\n",
    "    ensemble_preds = np.argmax(ensemble_predict_proba, axis=1)\n",
    "\n",
    "    return ensemble_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747204b5-5878-4bfa-a15f-38bf60403cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n",
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:01, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블의 검증 정확도: 0.9393627954779034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "# 모든 모델에 대해 반복\n",
    "for model_path in model_paths:\n",
    "    # 모델 생성\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_ckpt, num_labels=3)\n",
    "    # 모델 가중치 로드\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(args.device)\n",
    "    models.append(model)\n",
    "\n",
    "# 앙상블 예측\n",
    "ensemble_predictions = ensemble_predict(models, args, valid_dataloader)\n",
    "\n",
    "# 앙상블 예측의 정확도 계산\n",
    "accuracy = flat_accuracy(ensemble_predictions, valid_labels)\n",
    "print(f'앙상블의 검증 정확도: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf0cb9-a3c8-4293-a1e9-edca3eaa1bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
